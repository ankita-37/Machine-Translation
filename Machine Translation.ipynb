{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a machine translation model which takes English language as an input and return French Language as an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a function for loading our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English data\n",
    "english_sentences = load_data('data/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = load_data('data/small_vocab_fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Analysis & Pre-processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line in **small_vocab_en** contains an English sentence with the respective translation in each line of **small_vocab_fr**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the top 5 lines of both the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sample 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "French sample 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "\n",
      "English sample 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "French sample 2:  les Ã©tats-unis est gÃ©nÃ©ralement froid en juillet , et il gÃ¨le habituellement en novembre .\n",
      "\n",
      "English sample 3:  california is usually quiet during march , and it is usually hot in june .\n",
      "French sample 3:  california est gÃ©nÃ©ralement calme en mars , et il est gÃ©nÃ©ralement chaud en juin .\n",
      "\n",
      "English sample 4:  the united states is sometimes mild during june , and it is cold in september .\n",
      "French sample 4:  les Ã©tats-unis est parfois lÃ©gÃ¨re en juin , et il fait froid en septembre .\n",
      "\n",
      "English sample 5:  your least liked fruit is the grape , but my least liked is the apple .\n",
      "French sample 5:  votre moins aimÃ© fruit est le raisin , mais mon moins aimÃ© est la pomme .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(5):\n",
    "    print('English sample {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('French sample {}:  {}\\n'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can observe that the data is pretty much processed :\n",
    "\n",
    "1) All text is already in lower case      \n",
    "2) No punctuations are present          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complexity of the problem is determined by the complexity of the vocabulary. A more complex vocabulary is a more complex problem. Let's look at the complexity of the dataset we'll be working with.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "#English sentences\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "#French sentences\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is pretty much complex as seen from above observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we cannot directly use text data as an input to the model, we have to first tokenize our text data to get sequence of integers and then add a padding layer, in order to make the length of sequence of integers same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization & Padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for tokenization \n",
    "# Input  ---> List of sentences\n",
    "# Output ---> Tuple of tokenized data & tokenizer used\n",
    "\n",
    "def tokenize(x):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n",
    "\n",
    "\n",
    "\n",
    "## Function for padding\n",
    "# Input  ----> List of sequences, length to pad the sequence(here we assigned it as None, thus it will use length of longest sequence from the list of sequences)\n",
    "# Output ----> Padded numpy array of sequences\n",
    "\n",
    "def pad(x, length=None):\n",
    "    return pad_sequences(x, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create another function which will pre-process our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = preprocess(english_sentences, french_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the max sequence length of tokenized dataset and the vocab size for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 345\n"
     ]
    }
   ],
   "source": [
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "#Printing the same\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try different neural networks and check which model gives best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Simple RNN           \n",
    "2) RNN with Embedding             \n",
    "3) Bidirectional RNN               \n",
    "4) Combining all the above models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the models we have to create another function which will act as a bridge between the logits from the neural network to the French translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now start building the models :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Simple RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN Model\n",
    "\n",
    "def simple_rnn(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # Building the layers\n",
    "    model = Sequential()\n",
    "    model.add(GRU(256, input_shape=input_shape[1:], return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
    "\n",
    "    # Compiling model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 21, 256)           198912    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 21, 1024)          263168    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 21, 346)           354650    \n",
      "=================================================================\n",
      "Total params: 816,730\n",
      "Trainable params: 816,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "108/108 [==============================] - 249s 2s/step - loss: 2.6121 - accuracy: 0.4720 - val_loss: 1.3022 - val_accuracy: 0.6267\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 240s 2s/step - loss: 1.2944 - accuracy: 0.6293 - val_loss: 1.0962 - val_accuracy: 0.6665\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 179s 2s/step - loss: 1.1138 - accuracy: 0.6636 - val_loss: 0.9617 - val_accuracy: 0.6947\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 162s 2s/step - loss: 1.0100 - accuracy: 0.6819 - val_loss: 0.8971 - val_accuracy: 0.7121\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 158s 1s/step - loss: 0.9547 - accuracy: 0.6903 - val_loss: 0.8619 - val_accuracy: 0.7114\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 159s 1s/step - loss: 0.9087 - accuracy: 0.6983 - val_loss: 0.8073 - val_accuracy: 0.7256\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 199s 2s/step - loss: 0.8694 - accuracy: 0.7107 - val_loss: 0.7529 - val_accuracy: 0.7447\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 199s 2s/step - loss: 0.8236 - accuracy: 0.7234 - val_loss: 0.7247 - val_accuracy: 0.7559\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 195s 2s/step - loss: 0.7999 - accuracy: 0.7309 - val_loss: 0.7617 - val_accuracy: 0.7391\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 200s 2s/step - loss: 0.8171 - accuracy: 0.7196 - val_loss: 0.7138 - val_accuracy: 0.7559\n",
      "new jersey est parfois calme en mois de il et il est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_rnn(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size+1)\n",
    "\n",
    "print(simple_rnn_model.summary())\n",
    "\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print predictions\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "new jersey est parfois calme en mois de il et il est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct Translation:\n",
      "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
      "\n",
      "Original text:\n",
      "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[:1])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) RNN with Embedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding is a vector representation of the word that is close to similar words in n-dimensional space, where the n represents the size of the embedding vectors.\n",
    "\n",
    "In this model, we'll create a RNN model using embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN with embeddings\n",
    "\n",
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # Building the layers\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
    "    model.add(GRU(256, return_sequences=True))    \n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
    "\n",
    "    # Compiling the model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 21, 256)           51200     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 21, 256)           394752    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 21, 1024)          263168    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 21, 346)           354650    \n",
      "=================================================================\n",
      "Total params: 1,063,770\n",
      "Trainable params: 1,063,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "108/108 [==============================] - 222s 2s/step - loss: 2.3381 - accuracy: 0.5472 - val_loss: 0.4871 - val_accuracy: 0.8444\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 218s 2s/step - loss: 0.4584 - accuracy: 0.8513 - val_loss: 0.3099 - val_accuracy: 0.8961\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 215s 2s/step - loss: 0.3137 - accuracy: 0.8953 - val_loss: 0.2459 - val_accuracy: 0.9167\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 218s 2s/step - loss: 0.2539 - accuracy: 0.9139 - val_loss: 0.2181 - val_accuracy: 0.9250\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 218s 2s/step - loss: 0.2248 - accuracy: 0.9227 - val_loss: 0.2049 - val_accuracy: 0.9286\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 217s 2s/step - loss: 0.2068 - accuracy: 0.9278 - val_loss: 0.1981 - val_accuracy: 0.9315\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 215s 2s/step - loss: 0.1976 - accuracy: 0.9305 - val_loss: 0.1922 - val_accuracy: 0.9335\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 214s 2s/step - loss: 0.1874 - accuracy: 0.9335 - val_loss: 0.1865 - val_accuracy: 0.9354\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 214s 2s/step - loss: 0.1854 - accuracy: 0.9340 - val_loss: 0.1834 - val_accuracy: 0.9360\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 194s 2s/step - loss: 0.1808 - accuracy: 0.9354 - val_loss: 0.1828 - val_accuracy: 0.9356\n",
      "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "#Reshape the input\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "\n",
    "# Train the neural network\n",
    "embed_rnn_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "\n",
    "embed_rnn_model.summary()\n",
    "\n",
    "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print predictions\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got an accuracy of 0.9356 after adding embeddings which is pretty much good after from simple RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct Translation:\n",
      "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
      "\n",
      "Original text:\n",
      "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[:1])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Bidirectional RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One restriction of a RNN is that it can't see the future input, only the past. This is where bidirectional recurrent neural networks come in. They are able to see the future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a bidirectional RNN model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.003\n",
    "    \n",
    "    # Building the layers\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True), input_shape=input_shape[1:]))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
    "\n",
    "    # Compiling the model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 21, 256)           51200     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 21, 256)           394752    \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 21, 1024)          263168    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 21, 346)           354650    \n",
      "=================================================================\n",
      "Total params: 1,063,770\n",
      "Trainable params: 1,063,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "108/108 [==============================] - 221s 2s/step - loss: 2.3280 - accuracy: 0.5502 - val_loss: 0.5083 - val_accuracy: 0.8354\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 223s 2s/step - loss: 0.4706 - accuracy: 0.8476 - val_loss: 0.3178 - val_accuracy: 0.8939\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 217s 2s/step - loss: 0.3190 - accuracy: 0.8930 - val_loss: 0.2552 - val_accuracy: 0.9126\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 228s 2s/step - loss: 0.2577 - accuracy: 0.9124 - val_loss: 0.2199 - val_accuracy: 0.9249\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 202s 2s/step - loss: 0.2308 - accuracy: 0.9208 - val_loss: 0.2066 - val_accuracy: 0.9283\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 215s 2s/step - loss: 0.2107 - accuracy: 0.9271 - val_loss: 0.2025 - val_accuracy: 0.9298\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 221s 2s/step - loss: 0.2003 - accuracy: 0.9302 - val_loss: 0.1900 - val_accuracy: 0.9341\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 185s 2s/step - loss: 0.1901 - accuracy: 0.9330 - val_loss: 0.1845 - val_accuracy: 0.9347\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 192s 2s/step - loss: 0.1837 - accuracy: 0.9348 - val_loss: 0.1831 - val_accuracy: 0.9358\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 214s 2s/step - loss: 0.1819 - accuracy: 0.9353 - val_loss: 0.1814 - val_accuracy: 0.9365\n",
      "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "#Reshape the input\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "\n",
    "# Train and Print prediction\n",
    "embed_rnn_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "\n",
    "embed_rnn_model.summary()\n",
    "\n",
    "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct Translation:\n",
      "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
      "\n",
      "Original text:\n",
      "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[:1])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Custom Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a model that incorporates embedding and a bidirectional rnn into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.003\n",
    "    \n",
    "    # Building the layers    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding\n",
    "    model.add(Embedding(english_vocab_size, 128, input_length=input_shape[1],input_shape=input_shape[1:]))\n",
    "    \n",
    "    # Encoder\n",
    "    model.add(Bidirectional(GRU(128)))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    \n",
    "    # Decoder\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(512, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    \n",
    "    #Compiling the model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now predict the output using the custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 15, 128)           25600     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               198144    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 21, 256)           296448    \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 21, 512)           131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 21, 346)           177498    \n",
      "=================================================================\n",
      "Total params: 829,274\n",
      "Trainable params: 829,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "108/108 [==============================] - 309s 3s/step - loss: 3.4283 - accuracy: 0.3862 - val_loss: 1.6843 - val_accuracy: 0.5758\n",
      "Epoch 2/25\n",
      "108/108 [==============================] - 280s 3s/step - loss: 1.6239 - accuracy: 0.5794 - val_loss: 1.2065 - val_accuracy: 0.6736\n",
      "Epoch 3/25\n",
      "108/108 [==============================] - 283s 3s/step - loss: 1.2391 - accuracy: 0.6599 - val_loss: 0.9951 - val_accuracy: 0.7191\n",
      "Epoch 4/25\n",
      "108/108 [==============================] - 213s 2s/step - loss: 1.0481 - accuracy: 0.7002 - val_loss: 0.8685 - val_accuracy: 0.7417\n",
      "Epoch 5/25\n",
      "108/108 [==============================] - 239s 2s/step - loss: 0.9078 - accuracy: 0.7292 - val_loss: 0.7569 - val_accuracy: 0.7661\n",
      "Epoch 6/25\n",
      "108/108 [==============================] - 237s 2s/step - loss: 0.8547 - accuracy: 0.7395 - val_loss: 0.6185 - val_accuracy: 0.8061\n",
      "Epoch 7/25\n",
      "108/108 [==============================] - 248s 2s/step - loss: 0.6930 - accuracy: 0.7808 - val_loss: 0.5146 - val_accuracy: 0.8367\n",
      "Epoch 8/25\n",
      "108/108 [==============================] - 213s 2s/step - loss: 0.5628 - accuracy: 0.8206 - val_loss: 0.4205 - val_accuracy: 0.8700\n",
      "Epoch 9/25\n",
      "108/108 [==============================] - 227s 2s/step - loss: 0.5280 - accuracy: 0.8324 - val_loss: 0.3435 - val_accuracy: 0.8976\n",
      "Epoch 10/25\n",
      "108/108 [==============================] - 224s 2s/step - loss: 0.4186 - accuracy: 0.8697 - val_loss: 0.2563 - val_accuracy: 0.9286\n",
      "Epoch 11/25\n",
      "108/108 [==============================] - 237s 2s/step - loss: 0.3207 - accuracy: 0.9030 - val_loss: 0.2386 - val_accuracy: 0.9315\n",
      "Epoch 12/25\n",
      "108/108 [==============================] - 237s 2s/step - loss: 0.2760 - accuracy: 0.9167 - val_loss: 0.1783 - val_accuracy: 0.9497\n",
      "Epoch 13/25\n",
      "108/108 [==============================] - 234s 2s/step - loss: 0.2249 - accuracy: 0.9331 - val_loss: 0.1581 - val_accuracy: 0.9558\n",
      "Epoch 14/25\n",
      "108/108 [==============================] - 239s 2s/step - loss: 0.2002 - accuracy: 0.9406 - val_loss: 0.1426 - val_accuracy: 0.9591\n",
      "Epoch 15/25\n",
      "108/108 [==============================] - 237s 2s/step - loss: 0.1820 - accuracy: 0.9458 - val_loss: 0.1308 - val_accuracy: 0.9628\n",
      "Epoch 16/25\n",
      "108/108 [==============================] - 196s 2s/step - loss: 0.1624 - accuracy: 0.9516 - val_loss: 0.1209 - val_accuracy: 0.9655\n",
      "Epoch 17/25\n",
      "108/108 [==============================] - 192s 2s/step - loss: 0.1438 - accuracy: 0.9567 - val_loss: 0.1130 - val_accuracy: 0.9671\n",
      "Epoch 18/25\n",
      "108/108 [==============================] - 211s 2s/step - loss: 0.1340 - accuracy: 0.9600 - val_loss: 0.1062 - val_accuracy: 0.9696\n",
      "Epoch 19/25\n",
      "108/108 [==============================] - 220s 2s/step - loss: 0.1272 - accuracy: 0.9618 - val_loss: 0.1082 - val_accuracy: 0.9680\n",
      "Epoch 20/25\n",
      "108/108 [==============================] - 253s 2s/step - loss: 0.1210 - accuracy: 0.9633 - val_loss: 0.1063 - val_accuracy: 0.9697\n",
      "Epoch 21/25\n",
      "108/108 [==============================] - 254s 2s/step - loss: 0.1177 - accuracy: 0.9647 - val_loss: 0.0915 - val_accuracy: 0.9738\n",
      "Epoch 22/25\n",
      "108/108 [==============================] - 237s 2s/step - loss: 0.1013 - accuracy: 0.9690 - val_loss: 0.0899 - val_accuracy: 0.9745\n",
      "Epoch 23/25\n",
      "108/108 [==============================] - 236s 2s/step - loss: 0.1016 - accuracy: 0.9691 - val_loss: 0.1290 - val_accuracy: 0.9626\n",
      "Epoch 24/25\n",
      "108/108 [==============================] - 241s 2s/step - loss: 0.2519 - accuracy: 0.9253 - val_loss: 0.0928 - val_accuracy: 0.9735\n",
      "Epoch 25/25\n",
      "108/108 [==============================] - 235s 2s/step - loss: 0.1050 - accuracy: 0.9687 - val_loss: 0.0858 - val_accuracy: 0.9761\n",
      "WARNING:tensorflow:5 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000026103663EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Sample 1:\n",
      "il a vu un vieux camion jaune <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Il a vu un vieux camion jaune\n",
      "Sample 2:\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: English tokenizer\n",
    "    :param y_tk: French tokenizer\n",
    "    \"\"\"\n",
    "    # TODO: Train neural network using model_final\n",
    "    model = model_final(x.shape,y.shape[1],\n",
    "                        len(x_tk.word_index)+1,\n",
    "                        len(y_tk.word_index)+1)\n",
    "    model.summary()\n",
    "    model.fit(x, y, batch_size=1024, epochs=25, validation_split=0.2)\n",
    "\n",
    "    \n",
    "    ## DON'T EDIT ANYTHING BELOW THIS LINE\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "\n",
    "    sentence = 'he saw a old yellow truck'\n",
    "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "    sentences = np.array([sentence[0], x[0]])\n",
    "    predictions = model.predict(sentences, len(sentences))\n",
    "\n",
    "    print('Sample 1:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "    print('Il a vu un vieux camion jaune')\n",
    "    print('Sample 2:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
    "    print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))\n",
    "\n",
    "\n",
    "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got pretty much good accuracy in this case :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
